resources:
  accelerators: {A40:1} # We can use cheaper accelerators for 8B model.
  disk_size: 50  # Ensure model checkpoints can fit.
  disk_tier: best
  ports: 8000  # Expose to internet traffic.
  image_id: docker:mbilalkaust/vllm:0.10.0
  use_spot: false

envs:
  MODEL_NAME: microsoft/Phi-4-mini-instruct
  # HF_TOKEN: <your-huggingface-token>  # Change to your own huggingface token, or use --env to pass.

setup: |
  source ~/home/.vllm-venv/bin/activate

  # Install Gradio for web UI.
  pip install gradio openai

run: |
  source ~/home/.vllm-venv/bin/activate
  echo 'Starting vllm api server...'
  vllm serve $MODEL_NAME \
    --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \
    --port 8000 \
    --trust-remote-code \
    2>&1 | tee api_server.log &

  echo 'Waiting for vllm api server to start...'
  while ! `cat api_server.log | grep -q 'Application startup complete.'`; do sleep 1; done

  echo 'Starting gradio server...'
  git clone https://github.com/vllm-project/vllm.git || true
  pip install gradio openai
  python vllm/examples/online_serving/gradio_openai_chatbot_webserver.py \
    -m $MODEL_NAME \
    --port 8811 \
    --model-url http://localhost:8000/v1 