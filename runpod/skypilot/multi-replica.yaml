resources:
  accelerators: {A40:1} # We can use cheaper accelerators for 8B model.
  disk_size: 50  # Ensure model checkpoints can fit.
  disk_tier: best
  ports: 8000  # Expose to internet traffic.
  image_id: docker:mbilalkaust/vllm:0.10.0
  use_spot: false

service:
  replica_policy:
    min_replicas: 1
    max_replicas: 2
    target_qps_per_replica: 1
  # An actual request for readiness probe.
  readiness_probe:
    path: /v1/chat/completions
    post_data:
      model: $MODEL_NAME
      messages:
        - role: user
          content: What is Generative AI?
      max_tokens: 1024

envs:
  MODEL_NAME: microsoft/Phi-4-mini-instruct
  # HF_TOKEN: <your-huggingface-token>  # Change to your own huggingface token, or use --env to pass.

run: |
  source ~/home/.vllm-venv/bin/activate
  echo 'Starting vllm api server...'
  vllm serve $MODEL_NAME \
    --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \
    --port 8000 \
    --trust-remote-code 